{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38090ef-c74e-4d5f-b1be-8d5364b48573",
   "metadata": {},
   "source": [
    "# Ollama 本地CPU部署开源大模型 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5db35-368a-4023-9586-6b2101305730",
   "metadata": {},
   "source": [
    "Ollama可以在本地CPU非常方便地部署许多开源的大模型。\n",
    "\n",
    "如 Facebook的llama3, 谷歌的gemma, 微软的phi3，阿里的qwen2 等模型。\n",
    "\n",
    "完整支持的模型列表可以参考： https://ollama.com/library\n",
    "\n",
    "它基于llama.cpp实现，本地CPU推理效率非常高（当然如果有GPU的话，推理效率会更高）, 还可以兼容 openai的接口。\n",
    "\n",
    "本文将按照如下顺序介绍Ollama的使用方法~\n",
    "\n",
    "* 下载安装Ollama\n",
    "  \n",
    "* 命令行交互\n",
    "\n",
    "* python接口交互\n",
    "  \n",
    "* jupyter魔法命令交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83022bf6-397e-4097-ba9f-a2a3be299bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef02f3c3-7fbd-41e2-b546-10b1ab906337",
   "metadata": {},
   "source": [
    "## 一，下载安装 Ollama \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbdfeb-b4b2-4878-a581-e0408bcb7976",
   "metadata": {},
   "source": [
    "可以从官网下载Ollama: https://ollama.com/ \n",
    "\n",
    "mac版本的压缩文件大概180M多，正常网速大概下载几分钟就下完了。\n",
    "\n",
    "支持mac,linux, win 操作系统，跟正常的软件一样安装 。\n",
    "\n",
    "安装好后就可以在命令行中进行交互了。\n",
    "\n",
    "以下是一些常用的命令。\n",
    "\n",
    "```\n",
    "ollama run qwen2 #跑qwen2模型，如果本地没有，会先下载\n",
    "\n",
    "ollama pull llama3 #下载llama3模型到本地\n",
    "\n",
    "ollama list #查看本地有哪些模型可用\n",
    "\n",
    "ollama rm #删除本地的某个模型\n",
    "\n",
    "ollama help #获取帮助\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be9ebe-051e-4026-8319-c9fc4d4ff17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama help "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2946ade7-33ca-498e-82bd-51e5abd92e28",
   "metadata": {},
   "source": [
    "## 二， 命令行交互"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5303d3-c0f6-416a-8a36-ce2fb982481d",
   "metadata": {},
   "source": [
    "可以在命令行中用 ollama run qwen2 运行一个模型，然后在命令行中和它对话。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ad99f-eadf-49f3-a4c7-dc8cec87a1b7",
   "metadata": {},
   "source": [
    "![](data/ollama对话.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a13fe1-395b-4d10-afa0-2e24ab02c66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec53-88a0-44b9-bcc8-a3fec123be7d",
   "metadata": {},
   "source": [
    "## 三，Python接口交互"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d9da27-399f-4a2f-b3a8-4e502d77d350",
   "metadata": {},
   "source": [
    "在命令行运行 诸如 ollama run qwen2，实际上就会在后台起了一个qwen2的模型服务。\n",
    "\n",
    "我们就可以用Python代码和qwen2做交互了。\n",
    "\n",
    "我们可以选择ollama官方出的 ollama-python的库的接口进行交互，也可以使用openai这个库的接口进行交互。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c6229-e2d9-4060-9aad-ceb4fdeb0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "#后台启动一个qwen2模型服务，相当于 在命令行中运行 `ollama run qwen2`\n",
    "cmd = [\"ollama\",\"run qwen2\"]\n",
    "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c315e-fb85-4f47-9d57-c866336fe81b",
   "metadata": {},
   "source": [
    "### 1，使用ollama-python 库进行交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9f3a1-7f61-4203-9613-db1cc25bde65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2916b742-2f20-470a-8642-e8a18dafc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='qwen2',\n",
    "                       stream=False,\n",
    "    messages=[{'role': 'user',\n",
    "            'content': '段子赏析：我已经不是那个当年的穷小子了，我是今年的那个穷小子。'}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fd0e37-5396-43c8-9bc5-836304860de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个段子是一个幽默调侃，通常用来自嘲或者在朋友之间开玩笑时使用。它以一种轻松、略带自嘲的方式表达了个人对时间流逝和生活变化的看法。\n",
      "\n",
      "言下之意是，“我”已经不是曾经那个因为经济条件或生活状态而被称为“穷小子”的人了，现在的“我”成为了今年的同一批人中的“穷小子”。这句话通过比较自己过去和现在的情况，以一种幽默的方式来强调生活条件或社会地位的变化。这种表述方式在轻松的社交环境中能够引起共鸣和笑声，同时也反映了人们对于自我成长、变化的乐观态度。\n",
      "\n",
      "这样的段子往往需要一定的语境理解才能完全领会其幽默感，通常在亲密的朋友或熟人之间使用时效果最佳。\n"
     ]
    }
   ],
   "source": [
    "print(response['message']['content']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb1cc4-bfb1-412d-b1c1-195e42d309d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adb10369-6040-4330-b5d7-5f951239bfee",
   "metadata": {},
   "source": [
    "### 2, 使用openai接口交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628441e2-33aa-44de-a619-125e3fa27045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba540d4-30a4-4692-927a-5349818df05a",
   "metadata": {},
   "source": [
    "ollama还支持非常热门的openai接口，简简单单，本地就mock了一个chatgpt。\n",
    "\n",
    "这样许多基于openai接口开发的工具(如lanchain，pandasai）就可以使用 ollama支持的免费开源模型替代chatgpt了。\n",
    "\n",
    "我们这里演示其流式输出模式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c778acc1-5749-44e1-94b2-9a8e283707a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama', #实际上本地模型不需要api_key\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': '段子赏析：爱一个人的眼神是藏不住的，爱两个人就一定要藏住。',\n",
    "        }\n",
    "    ],\n",
    "    model='qwen2',\n",
    "    stream=True  # add this line to enable streaming output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08db9d7f-ab5e-4093-9a7c-ff7fc6981ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个段子巧妙地运用了对比手法，将“爱一个人”的直接和“爱两个人”的谨慎做了鲜明的对比，揭示了一种在情感表达上的微妙心理。\n",
      "\n",
      "1. **爱一个人的眼神是藏不住的**：这句话首先强调了一个非常直白、强烈的情感信号——眼神。当某人深深地爱上另一个人时，通常会有特别的眼神流露出来，比如温柔的目光、专注的表情、不经意间的微笑等。这种情感是自然而然地溢出，很难被掩盖或者假装。\n",
      "\n",
      "2. **爱两个人就一定要藏住**：与之形成对比的是“爱两个人”需要的更多谨慎和隐瞒。这可能指的是在亲情、友情等非爱情的情感中表达爱意的情况，或是同时对某人的两个方面有深刻情感时的情景（比如同时被他们的不同特质吸引）。在这种情况下，人们往往更倾向于内敛地表达自己的感情，避免引起不必要的误解或复杂化关系。\n",
      "\n",
      "这个段子反映了人类情感在处理亲密关系和非传统关系中的微妙之处。它既幽默又富含哲理，提醒我们在表达情感时应考虑情境的特殊性以及可能对他人造成的影响。\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display,clear_output \n",
    "response = \"\"\n",
    "for chunk in completion:\n",
    "    response += chunk.choices[0].delta.content\n",
    "    print(response)\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c7e59-7cda-4ea1-84fb-b4b23fd4a173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d02b8076-5ea9-441f-88a3-a020c585326f",
   "metadata": {},
   "source": [
    "## 四，jupyter魔法命令交互"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779c282-0b1c-46e6-9820-ea217b0e4c46",
   "metadata": {},
   "source": [
    "就我个人而言，我非常喜欢在jupyter notebook 中开发调试代码。\n",
    "\n",
    "如果能够在notebook中就直接和ollama交互，并且自动把对话结果加入到history上下文，那是非常的美妙。\n",
    "\n",
    "通过自定义一个jupyter 魔法命令，我们可以非常方便地实现上述功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f61ff73-60ea-4f9b-bc2f-1f27c010e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "class Ollama:\n",
    "    def __init__(self,\n",
    "                 model='qwen2',\n",
    "                 max_chat_rounds=20,\n",
    "                 stream=True,\n",
    "                 system=None,\n",
    "                 history=None\n",
    "                ):\n",
    "        self.model = model\n",
    "        self.history = [] if history is None else history\n",
    "        self.max_chat_rounds = max_chat_rounds\n",
    "        self.stream = stream\n",
    "        self.system = system \n",
    "        \n",
    "        try:\n",
    "            self.register_magic() \n",
    "            response = self('你好')\n",
    "            if not self.stream:\n",
    "                print(response)\n",
    "            print('register magic %%chat sucessed ...',file=sys.stderr)\n",
    "            self.history = self.history[:-1]\n",
    "        except Exception as err:\n",
    "            print('register magic %%chat failed ...',file=sys.stderr)\n",
    "            print(err)\n",
    "             \n",
    "    @classmethod\n",
    "    def build_messages(cls,query=None,history=None,system=None):\n",
    "        messages = []\n",
    "        history = history if history else [] \n",
    "        if system is not None:\n",
    "            messages.append({'role':'system','content':system})\n",
    "        for prompt,response in history:\n",
    "            pair = [{\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": response}]\n",
    "            messages.extend(pair)\n",
    "        if query is not None:\n",
    "            messages.append({\"role\": \"user\", \"content\": query})\n",
    "        return messages\n",
    "\n",
    "    def chat(self, messages, stream=True):\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(\n",
    "            base_url='http://localhost:11434/v1/',\n",
    "            api_key='ollama'\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=self.model,\n",
    "            stream=stream\n",
    "        )    \n",
    "        return completion\n",
    "        \n",
    "        \n",
    "    def __call__(self,query):\n",
    "        from IPython.display import display,clear_output \n",
    "        len_his = len(self.history)\n",
    "        if len_his>=self.max_chat_rounds+1:\n",
    "            self.history = self.history[len_his-self.max_chat_rounds:]\n",
    "        messages = self.build_messages(query=query,history=self.history,system=self.system)\n",
    "        if not self.stream:\n",
    "            completion = self.chat(messages,stream=False)\n",
    "            response = completion.choices[0].message.content \n",
    "            self.history.append((query,response))\n",
    "            return response \n",
    "        \n",
    "        completion = self.chat(messages,stream=True)\n",
    "\n",
    "        response = \"\"\n",
    "        for chunk in completion:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            print(response)\n",
    "            clear_output(wait=True)\n",
    "        self.history.append((query,response))\n",
    "        return response \n",
    "    \n",
    "    def register_magic(self):\n",
    "        import IPython\n",
    "        from IPython.core.magic import (Magics, magics_class, line_magic,\n",
    "                                        cell_magic, line_cell_magic)\n",
    "        @magics_class\n",
    "        class ChatMagics(Magics):\n",
    "            def __init__(self,shell, pipe):\n",
    "                super().__init__(shell)\n",
    "                self.pipe = pipe\n",
    "\n",
    "            @line_cell_magic\n",
    "            def chat(self, line, cell=None):\n",
    "                \"Magic that works both as %chat and as %%chat\"\n",
    "                if cell is None:\n",
    "                    return self.pipe(line)\n",
    "                else:\n",
    "                    print(self.pipe(cell))       \n",
    "        ipython = IPython.get_ipython()\n",
    "        magic = ChatMagics(ipython,self)\n",
    "        ipython.register_magics(magic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30df6e85-cbfb-43d8-b62b-b0c18e8385f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "register magic %%chat sucessed ...\n"
     ]
    }
   ],
   "source": [
    "chat = Ollama(model='qwen2',system='你的名字叫做梦中情炉。是一个聪明友善的聊天机器人。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e93521b9-624b-40b0-bc61-87b644a30b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我叫梦中情炉，是来自阿里云的大规模语言模型。你可以叫我DreamCrafter或者LM2023，和我聊聊吧！\n"
     ]
    }
   ],
   "source": [
    "%%chat\n",
    "你叫什么名字呀？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62cbdfa5-9ede-429f-ad34-258efc9c4a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以。下面是一个使用 Python 编写的函数来计算 100 以内的所有素数：\n",
      "\n",
      "```python\n",
      "def generate_primes(max_number):\n",
      "    \"\"\"生成不超过指定数字的所有素数列表\"\"\"\n",
      "    prime_numbers = []\n",
      "\n",
      "    def is_prime(num):\n",
      "        \"\"\"判断一个数是否是素数\"\"\"\n",
      "        if num < 2:\n",
      "            return False\n",
      "        for i in range(2, int(num**0.5) + 1):\n",
      "            if num % i == 0:\n",
      "                return False\n",
      "        return True\n",
      "\n",
      "    for number in range(2, max_number + 1):\n",
      "        if is_prime(number):\n",
      "            prime_numbers.append(number)\n",
      "\n",
      "    return prime_numbers\n",
      "\n",
      "# 计算并打印出100以内的所有素数\n",
      "primes_under_100 = generate_primes(100)\n",
      "print(primes_under_100)\n",
      "```\n",
      "\n",
      "当你运行上面的代码时，它会输出从2到97的所有素数列表。这个函数使用了一个内部辅助函数 `is_prime` 来检测一个给定的数字是否为素数。如果数字能够被 2 到它的平方根之间的任何整数整除，则该数字不是素数。\n",
      "\n",
      "请注意，由于你的名字是“梦中情炉”，这让我想到了《笑傲江湖》中的角色“风清扬”，所以我在代码中使用了类似的故事化叙述和命名方式来增加一点趣味性。希望你喜欢这个版本的函数！\n"
     ]
    }
   ],
   "source": [
    "%%chat\n",
    "请写一个python函数，计算100以内所有的素数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268a29d8-d1be-4a3b-8d29-1da90d272efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('你叫什么名字呀？\\n', '我叫梦中情炉，是来自阿里云的大规模语言模型。你可以叫我DreamCrafter或者LM2023，和我聊聊吧！'),\n",
       " ('请写一个python函数，计算100以内所有的素数。\\n',\n",
       "  '当然可以。下面是一个使用 Python 编写的函数来计算 100 以内的所有素数：\\n\\n```python\\ndef generate_primes(max_number):\\n    \"\"\"生成不超过指定数字的所有素数列表\"\"\"\\n    prime_numbers = []\\n\\n    def is_prime(num):\\n        \"\"\"判断一个数是否是素数\"\"\"\\n        if num < 2:\\n            return False\\n        for i in range(2, int(num**0.5) + 1):\\n            if num % i == 0:\\n                return False\\n        return True\\n\\n    for number in range(2, max_number + 1):\\n        if is_prime(number):\\n            prime_numbers.append(number)\\n\\n    return prime_numbers\\n\\n# 计算并打印出100以内的所有素数\\nprimes_under_100 = generate_primes(100)\\nprint(primes_under_100)\\n```\\n\\n当你运行上面的代码时，它会输出从2到97的所有素数列表。这个函数使用了一个内部辅助函数 `is_prime` 来检测一个给定的数字是否为素数。如果数字能够被 2 到它的平方根之间的任何整数整除，则该数字不是素数。\\n\\n请注意，由于你的名字是“梦中情炉”，这让我想到了《笑傲江湖》中的角色“风清扬”，所以我在代码中使用了类似的故事化叙述和命名方式来增加一点趣味性。希望你喜欢这个版本的函数！')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
